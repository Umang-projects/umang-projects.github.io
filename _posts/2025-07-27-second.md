---
title: "ğŸš€HyLoRA: Hybrid Low-Rank Adaptation for Efficient Model Compression and Fine-Tuning"
categories: [ModelÂ Compression, Lowâ€‘RankÂ Adaptation]
tags: [HyLoRA, SVD, LoRA, LLaMA, Perplexity, PerformanceÂ Tradeâ€‘off, Fineâ€‘Tuning]
---

---
HyLoRA is a novel technique for compressing large language models using a hybrid of SVD (for static compression) and LoRA (for dynamic fine-tuning). This project demonstrates a complete pipeline to compress and fine-tune a LLaMA-based model (1.3B) using PyTorch with minimal performance loss.

---

## ğŸ¯ Goal

Achieve significant compression while retaining downstream performance (perplexity) using a hybrid method.

---

## ğŸ”¬ The Engineering Journey

### Phase 1: The "AutoRank" Paradox ğŸ”

**The Initial Goal**: Build an intelligent AutoRank optimizer that automatically determines the best SVD rank for compression.

**The Investigation**: Discovered that naive SVD compression often results in a *larger* model due to the extra parameters introduced by the SVD layers. This led to models being bigger than the original if ranks werenâ€™t constrained.

### Phase 2: Perplexity Collapse ğŸ’¥

**Challenge**: Even a mathematically correct compression of just 0.59% caused the model's perplexity to explode from 18.77 â†’ 244!

**Insight**: This provided empirical proof of cascading error collapse, where tiny, independent errors at each layer compound exponentially in deep architectures.

**Discovery**: Static, "greedy" compression is fundamentally insufficient for deep models. A mechanism for performance recovery is essential.

---

## âš™ï¸ Architectural Deep Dive: The `SVD_LoRA_Linear` Layer

This custom layer combines a frozen SVD base with trainable LoRA adapters. The forward pass is:

```
y = ((x @ SVh.T) @ U.T) + b              # Frozen SVD Base
  + alpha * ((x @ A.T) @ B.T)            # Trainable LoRA Adapter
```

Where:

* `x`: Input
* `SVh`, `U`: Frozen SVD components from initial compression
* `A`, `B`: Trainable LoRA adapters (learned during fine-tuning)
* `alpha`: Scaling factor for LoRA

This allows SVD to compress and LoRA to adapt.

---

### ğŸ¯ Performance Recovery via Fine-Tuning

The LoRA adapters are optimized using a standard causal language modeling objective:

```
L(Î¸_LoRA) = -âˆ‘ log p(w_i | w_<i ; Î¸_SVD, Î¸_LoRA)
```

Here, only the LoRA parameters `Î¸_LoRA` are updated while the SVD base `Î¸_SVD` remains frozen.

This lets the model recover from information lost during compression, using only \~0.5% of the original parameters.

---

## ğŸ† Breakthrough Results

Applied to **TinyLlama-1.1B-Chat-v1.0**, our method demonstrates a remarkable size-performance trade-off:

| Model Configuration                   | Parameters (M) | Size (MB)   | Perplexity (PPL) |
| :------------------------------------ | :------------- | :---------- | :--------------- |
| **1. Original Baseline**              | 1100.05        | 2098.18     | **18.77**        |
| **2. SVD Compressed (Î±=0.7)**         | 529.07         | 1009.11     | **339.93**       |
| **3. HyLoRA (SVD Î±=0.7 + LoRA r=32)** | **538.98**     | **1028.05** | **28.63**        |

---

## ğŸ“Š Performance Analysis

The final model's performance is highly sensitive to the initial SVD compression ratio (`alpha`) and the capacity of the LoRA adapter (`rank`). We performed a systematic sweep to find the optimal configuration.

| SVD Alpha (Î±) | LoRA Rank (r) | Final Size (MB) | Pre-Tune PPL | **Post-Tune PPL** |
| :-----------: | :-----------: | :-------------: | :----------: | :---------------: |
|      0.5      |       16      |      \~753      |    828.86    |       50.02       |
|      0.5      |       32      |      \~772      |    828.86    |       49.84       |
|      0.7      |       16      |      \~1009     |    339.93    |       29.14       |
|    **0.7**    |     **32**    |    **\~1028**   |  **339.93**  |     **28.63**     |

**Optimal Configuration:** An SVD `alpha` of **0.7** combined with a LoRA rank of **32** provides the best balance, achieving a perplexity score very close to the original baseline while still offering a massive **\~51% reduction** in the final model's size.

---

> ### ğŸ–¼ï¸ Compression vs. Performance Trade-off Frontier

<p align="center">
  <img src="perf_metrics vs Param_cost.png" alt="HyLoRA Tradeoff" width="700"/>
</p>

---

## ğŸ”§ Core Components

### ğŸ“ `svd_lora_layer.py`

Defines `SVD_LoRA_Linear`, the hybrid layer replacing standard `nn.Linear`.

### ğŸ§ª `finetune.py`

Performs LoRA-based fine-tuning on top of the frozen SVD-compressed model.

### ğŸ§¾ `evaluate.py`

Reports perplexity (PPL) and model size.

---

## ğŸ“‰ Results Summary

### HyLoRA achieves a powerful balance between size and performance:

- ğŸ’¾ **Over 2x compression**, reducing the final model from 1.1B to 539M parameters (**51% parameter reduction**).
- ğŸ¯ **Recovers from a catastrophic ~1700% perplexity increase** (from the SVD base) down to a final degradation of only **+53%** versus the original model.
- ğŸ”§ **Achieves this recovery by fine-tuning only 19.82M parameters** (just 1.8% of the original model's size).

---

## ğŸ§  Insights

* SVD-only can *increase* size without careful rank constraints.
* LoRA can recover performance at extremely low cost.
* The hybrid architecture provides a principled, extensible way to combine compression and adaptation.


## ğŸ”® Future Directions

1. **Architecturally-Aware Compression** ğŸ—ï¸

   ```mermaid
   graph LR
   A[MLP Layers] -->|Higher Compression| B[Optimal]
   C[Attention Layers] -->|Lower Compression| B
   ```

2. **Automated Compression Budgeting** ğŸ¤–

   * Layer-adaptive alpha selection ğŸ›ï¸
   * Perplexity-aware rank optimization ğŸ“Š

3. **Hybrid Quantization** âš¡

   * 4-bit quantization (bitsandbytes) ğŸ”¢
   * FP8 precision formats ğŸ¯

4. **Hardware Optimization** ğŸ›¡ï¸

   * TensorRT deployment pipeline ğŸš€
   * CUDA-optimized kernels ğŸ–¥ï¸

## ğŸ“š Acknowledgements ğŸ™

* **TinyLlama Team** for their powerful open-source model ğŸ‘
* **Hugging Face** for Transformers and Datasets libraries ğŸ¤—
* **LoRA** authors for foundational adapter research ğŸ§©
* **SVD** pioneers for mathematical foundations of compression â—

---

```bash
Original: 2098 MB, PPL 18.77 â†’ Compressed: 772 MB, PPL 28.63
2.72x smaller with only 1.53x perplexity increase ğŸŒŸ
```


