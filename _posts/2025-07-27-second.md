---
title: "🚀HyLoRA: Hybrid Low-Rank Adaptation for Efficient Model Compression and Fine-Tuning"
categories: [Model Compression, Low‑Rank Adaptation]
tags: [HyLoRA, SVD, LoRA, LLaMA, Perplexity, Performance Trade‑off, Fine‑Tuning]
---

---
HyLoRA is a novel technique for compressing large language models using a hybrid of SVD (for static compression) and LoRA (for dynamic fine-tuning). This project demonstrates a complete pipeline to compress and fine-tune a LLaMA-based model (1.3B) using PyTorch with minimal performance loss.

---

## 🎯 Goal

Achieve significant compression while retaining downstream performance (perplexity) using a hybrid method.

---

## 🔬 The Engineering Journey

### Phase 1: The "AutoRank" Paradox 🔍

**The Initial Goal**: Build an intelligent AutoRank optimizer that automatically determines the best SVD rank for compression.

**The Investigation**: Discovered that naive SVD compression often results in a *larger* model due to the extra parameters introduced by the SVD layers. This led to models being bigger than the original if ranks weren’t constrained.

### Phase 2: Perplexity Collapse 💥

**Challenge**: Even a mathematically correct compression of just 0.59% caused the model's perplexity to explode from 18.77 → 244!

**Insight**: This provided empirical proof of cascading error collapse, where tiny, independent errors at each layer compound exponentially in deep architectures.

**Discovery**: Static, "greedy" compression is fundamentally insufficient for deep models. A mechanism for performance recovery is essential.

---

## ⚙️ Architectural Deep Dive: The `SVD_LoRA_Linear` Layer

This custom layer combines a frozen SVD base with trainable LoRA adapters. The forward pass is:

```
y = ((x @ SVh.T) @ U.T) + b              # Frozen SVD Base
  + alpha * ((x @ A.T) @ B.T)            # Trainable LoRA Adapter
```

Where:

* `x`: Input
* `SVh`, `U`: Frozen SVD components from initial compression
* `A`, `B`: Trainable LoRA adapters (learned during fine-tuning)
* `alpha`: Scaling factor for LoRA

This allows SVD to compress and LoRA to adapt.

---

### 🎯 Performance Recovery via Fine-Tuning

The LoRA adapters are optimized using a standard causal language modeling objective:

```
L(θ_LoRA) = -∑ log p(w_i | w_<i ; θ_SVD, θ_LoRA)
```

Here, only the LoRA parameters `θ_LoRA` are updated while the SVD base `θ_SVD` remains frozen.

This lets the model recover from information lost during compression, using only \~0.5% of the original parameters.

---

## 🏆 Breakthrough Results

Applied to **TinyLlama-1.1B-Chat-v1.0**, our method demonstrates a remarkable size-performance trade-off:

| Model Configuration                   | Parameters (M) | Size (MB)   | Perplexity (PPL) |
| :------------------------------------ | :------------- | :---------- | :--------------- |
| **1. Original Baseline**              | 1100.05        | 2098.18     | **18.77**        |
| **2. SVD Compressed (α=0.7)**         | 529.07         | 1009.11     | **339.93**       |
| **3. HyLoRA (SVD α=0.7 + LoRA r=32)** | **538.98**     | **1028.05** | **28.63**        |

---

## 📊 Performance Analysis

The final model's performance is highly sensitive to the initial SVD compression ratio (`alpha`) and the capacity of the LoRA adapter (`rank`). We performed a systematic sweep to find the optimal configuration.

| SVD Alpha (α) | LoRA Rank (r) | Final Size (MB) | Pre-Tune PPL | **Post-Tune PPL** |
| :-----------: | :-----------: | :-------------: | :----------: | :---------------: |
|      0.5      |       16      |      \~753      |    828.86    |       50.02       |
|      0.5      |       32      |      \~772      |    828.86    |       49.84       |
|      0.7      |       16      |      \~1009     |    339.93    |       29.14       |
|    **0.7**    |     **32**    |    **\~1028**   |  **339.93**  |     **28.63**     |

**Optimal Configuration:** An SVD `alpha` of **0.7** combined with a LoRA rank of **32** provides the best balance, achieving a perplexity score very close to the original baseline while still offering a massive **\~51% reduction** in the final model's size.

---

> ### 🖼️ Compression vs. Performance Trade-off Frontier

<p align="center">
  <img src="perf_metrics vs Param_cost.png" alt="HyLoRA Tradeoff" width="700"/>
</p>

---

## 🔧 Core Components

### 📁 `svd_lora_layer.py`

Defines `SVD_LoRA_Linear`, the hybrid layer replacing standard `nn.Linear`.

### 🧪 `finetune.py`

Performs LoRA-based fine-tuning on top of the frozen SVD-compressed model.

### 🧾 `evaluate.py`

Reports perplexity (PPL) and model size.

---

## 📉 Results Summary

### HyLoRA achieves a powerful balance between size and performance:

- 💾 **Over 2x compression**, reducing the final model from 1.1B to 539M parameters (**51% parameter reduction**).
- 🎯 **Recovers from a catastrophic ~1700% perplexity increase** (from the SVD base) down to a final degradation of only **+53%** versus the original model.
- 🔧 **Achieves this recovery by fine-tuning only 19.82M parameters** (just 1.8% of the original model's size).

---

## 🧠 Insights

* SVD-only can *increase* size without careful rank constraints.
* LoRA can recover performance at extremely low cost.
* The hybrid architecture provides a principled, extensible way to combine compression and adaptation.


## 🔮 Future Directions

1. **Architecturally-Aware Compression** 🏗️

   ```mermaid
   graph LR
   A[MLP Layers] -->|Higher Compression| B[Optimal]
   C[Attention Layers] -->|Lower Compression| B
   ```

2. **Automated Compression Budgeting** 🤖

   * Layer-adaptive alpha selection 🎛️
   * Perplexity-aware rank optimization 📊

3. **Hybrid Quantization** ⚡

   * 4-bit quantization (bitsandbytes) 🔢
   * FP8 precision formats 🎯

4. **Hardware Optimization** 🛡️

   * TensorRT deployment pipeline 🚀
   * CUDA-optimized kernels 🖥️

## 📚 Acknowledgements 🙏

* **TinyLlama Team** for their powerful open-source model 🐑
* **Hugging Face** for Transformers and Datasets libraries 🤗
* **LoRA** authors for foundational adapter research 🧩
* **SVD** pioneers for mathematical foundations of compression ➗

---

```bash
Original: 2098 MB, PPL 18.77 → Compressed: 772 MB, PPL 28.63
2.72x smaller with only 1.53x perplexity increase 🌟
```


